{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import gensim\n",
    "import pandas as pd \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(all_docs):\n",
    "    no_punctuation_docs = []\n",
    "    for doc in all_docs:\n",
    "        doc = doc.replace(\"-\", \" \")\n",
    "        no_punctuation_docs.append(re.sub(r'[^\\w\\s]', '', doc))\n",
    "    return no_punctuation_docs\n",
    "\n",
    "\n",
    "def remove_names(all_docs):\n",
    "    no_name_docs = []\n",
    "    for doc in all_docs:\n",
    "        no_name_docs.append(' '.join([w for w, t in pos_tag(doc.split()) if t != 'NNP' and t != 'NNPS']))\n",
    "    return no_name_docs\n",
    "\n",
    "\n",
    "def tokenizer(all_docs):\n",
    "    tokenized_docs = []\n",
    "    for doc in all_docs:\n",
    "        tokenized_docs.append(word_tokenize(doc))\n",
    "    return tokenized_docs\n",
    "\n",
    "\n",
    "def lemmatizer(all_docs):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_docs = []\n",
    "    for doc in all_docs:\n",
    "        temp = []\n",
    "        for token in doc:\n",
    "            if token.isalpha():\n",
    "                temp.append(wordnet_lemmatizer.lemmatize(token, \"v\"))\n",
    "        lemmatized_docs.append(temp)\n",
    "    return lemmatized_docs\n",
    "\n",
    "def untokenizer(all_docs):\n",
    "    untokenized_docs = []\n",
    "    for doc in all_docs:\n",
    "        untokenized_docs.append(\" \".join(doc))\n",
    "    return untokenized_docs\n",
    "\n",
    "\n",
    "def fetch_stop_words():\n",
    "    stop_words = stopwords.words('english')\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def create_dtm(all_docs, stopwords, ngram):\n",
    "    # mdf = min(len(all_docs), 5)\n",
    "    vectorizer = CountVectorizer(lowercase=True, min_df=0.0, max_df=1.0, ngram_range=(1, ngram), stop_words=stopwords)\n",
    "    dtm = vectorizer.fit_transform(all_docs)\n",
    "    return vectorizer, dtm\n",
    "\n",
    "\n",
    "def tfidf_transformer(dtm):\n",
    "    tf_transformer = TfidfTransformer()\n",
    "    tfidf = tf_transformer.fit_transform(dtm)\n",
    "    return tf_transformer, tfidf\n",
    "\n",
    "\n",
    "def generate_sentence_vector(tokens, model, vectorizer, tfidf_dense):\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    for token in tokens:\n",
    "        if token in model.wv.vocab and token in vectorizer.vocabulary_:\n",
    "            vector = vector + model.wv[token] * tfidf_dense[0, vectorizer.vocabulary_[token]]\n",
    "    return vector\n",
    "\n",
    "\n",
    "def list_sample(list, n=5):\n",
    "    p = min(len(list), n)\n",
    "    for idx in range(0, p):\n",
    "        print(idx, list[idx])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Ah alright Good point! But neither does any other company. He gives them stock tho and 401k into his multibillion dollar companies. If you look at the stock market. 1 Tesla stock went up $700 this year.\n",
      "1    Breaking news! It is a cool annual giveaway ! 1oooooooo DOGE 5ooo BTC will be distributed among everyone who takes part in this event. #doge #dogecoin #Ethereum #Bitcoin #ETH #BTC Join here➜.tesla-giveawayx10.c om\n",
      "2 TITS EVERYONE- ELON IS MAKIMG A TITS JOKE OMG SO FUNNY! $TSLA \n",
      "3 RT : I’ve talked to a few instit PMs about $TSLA the past few days. Apparently none of the analysts are assuming global EV adop…\n",
      "4 RT : Bet Hertz runs a Superbowl ad for $TSLA fleet.\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "data = pd.read_csv('TwitterData_latest.csv')\n",
    "all_docs = data['text_clean']\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=all_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Ah alright Good point But neither does any other company He gives them stock tho and 401k into his multibillion dollar companies If you look at the stock market 1 Tesla stock went up 700 this year\n",
      "1    Breaking news It is a cool annual giveaway  1oooooooo DOGE 5ooo BTC will be distributed among everyone who takes part in this event doge dogecoin Ethereum Bitcoin ETH BTC Join heretesla giveawayx10c om\n",
      "2 TITS EVERYONE  ELON IS MAKIMG A TITS JOKE OMG SO FUNNY TSLA \n",
      "3 RT  Ive talked to a few instit PMs about TSLA the past few days Apparently none of the analysts are assuming global EV adop\n",
      "4 RT  Bet Hertz runs a Superbowl ad for TSLA fleet\n"
     ]
    }
   ],
   "source": [
    "# Removing Punctuation\n",
    "no_punctuation_docs = remove_punctuation(all_docs)\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=no_punctuation_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['Ah', 'alright', 'Good', 'point', 'But', 'neither', 'does', 'any', 'other', 'company', 'He', 'gives', 'them', 'stock', 'tho', 'and', '401k', 'into', 'his', 'multibillion', 'dollar', 'companies', 'If', 'you', 'look', 'at', 'the', 'stock', 'market', '1', 'Tesla', 'stock', 'went', 'up', '700', 'this', 'year']\n",
      "1 ['Breaking', 'news', 'It', 'is', 'a', 'cool', 'annual', 'giveaway', '1oooooooo', 'DOGE', '5ooo', 'BTC', 'will', 'be', 'distributed', 'among', 'everyone', 'who', 'takes', 'part', 'in', 'this', 'event', 'doge', 'dogecoin', 'Ethereum', 'Bitcoin', 'ETH', 'BTC', 'Join', 'heretesla', 'giveawayx10c', 'om']\n",
      "2 ['TITS', 'EVERYONE', 'ELON', 'IS', 'MAKIMG', 'A', 'TITS', 'JOKE', 'OMG', 'SO', 'FUNNY', 'TSLA']\n",
      "3 ['RT', 'Ive', 'talked', 'to', 'a', 'few', 'instit', 'PMs', 'about', 'TSLA', 'the', 'past', 'few', 'days', 'Apparently', 'none', 'of', 'the', 'analysts', 'are', 'assuming', 'global', 'EV', 'adop']\n",
      "4 ['RT', 'Bet', 'Hertz', 'runs', 'a', 'Superbowl', 'ad', 'for', 'TSLA', 'fleet']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- TOKENIZE -------------------------------------------------------------------\n",
    "\n",
    "# Tokenize each tweet\n",
    "tokenized_docs = tokenizer(no_punctuation_docs)\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['Ah', 'alright', 'Good', 'point', 'But', 'neither', 'do', 'any', 'other', 'company', 'He', 'give', 'them', 'stock', 'tho', 'and', 'into', 'his', 'multibillion', 'dollar', 'company', 'If', 'you', 'look', 'at', 'the', 'stock', 'market', 'Tesla', 'stock', 'go', 'up', 'this', 'year']\n",
      "1 ['Breaking', 'news', 'It', 'be', 'a', 'cool', 'annual', 'giveaway', 'DOGE', 'BTC', 'will', 'be', 'distribute', 'among', 'everyone', 'who', 'take', 'part', 'in', 'this', 'event', 'doge', 'dogecoin', 'Ethereum', 'Bitcoin', 'ETH', 'BTC', 'Join', 'heretesla', 'om']\n",
      "2 ['TITS', 'EVERYONE', 'ELON', 'IS', 'MAKIMG', 'A', 'TITS', 'JOKE', 'OMG', 'SO', 'FUNNY', 'TSLA']\n",
      "3 ['RT', 'Ive', 'talk', 'to', 'a', 'few', 'instit', 'PMs', 'about', 'TSLA', 'the', 'past', 'few', 'days', 'Apparently', 'none', 'of', 'the', 'analysts', 'be', 'assume', 'global', 'EV', 'adop']\n",
      "4 ['RT', 'Bet', 'Hertz', 'run', 'a', 'Superbowl', 'ad', 'for', 'TSLA', 'fleet']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- LEMMATIZE ------------------------------------------------------------------\n",
    "\n",
    "# lemmatize the tokens\n",
    "lemmatized_docs = lemmatizer(tokenized_docs)\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=lemmatized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Ah alright Good point But neither do any other company He give them stock tho and into his multibillion dollar company If you look at the stock market Tesla stock go up this year\n",
      "1 Breaking news It be a cool annual giveaway DOGE BTC will be distribute among everyone who take part in this event doge dogecoin Ethereum Bitcoin ETH BTC Join heretesla om\n",
      "2 TITS EVERYONE ELON IS MAKIMG A TITS JOKE OMG SO FUNNY TSLA\n",
      "3 RT Ive talk to a few instit PMs about TSLA the past few days Apparently none of the analysts be assume global EV adop\n",
      "4 RT Bet Hertz run a Superbowl ad for TSLA fleet\n"
     ]
    }
   ],
   "source": [
    "# Untokenize the tokens to form sentence again\n",
    "untokenized_docs = untokenizer(lemmatized_docs)\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=untokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- STOPWORDS ------------------------------------------------------------------\n",
    "\n",
    "# Fetch stopwords from custom list\n",
    "stop_words = fetch_stop_words()\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(min_df=0.0,\n",
      "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
      "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
      "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
      "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
      "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
      "                            'itself', ...])\n",
      "0 aa\n",
      "1 aaa\n",
      "2 aaaaaand\n",
      "3 aaaaah\n",
      "4 aaaaand\n",
      "(171454, 43266)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- VECTORIZE ------------------------------------------------------------------\n",
    "# Vectorize words\n",
    "vectorizer, dtm = create_dtm(untokenized_docs, stop_words, 1)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "if verbose:\n",
    "    print(vectorizer)\n",
    "    list_sample(list=feature_names)\n",
    "    print(dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfTransformer()\n",
      "(171454, 43266)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- TFIDF ----------------------------------------------------------------------\n",
    "\n",
    "# tfidf transformation\n",
    "tf_transformer, tfidf = tfidf_transformer(dtm)\n",
    "\n",
    "if verbose:\n",
    "    print(tf_transformer)\n",
    "    print(tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['ah' 'alright' 'company' 'dollar' 'give' 'go' 'good' 'look' 'market'\n",
      " 'multibillion' 'neither' 'point' 'stock' 'tesla' 'tho' 'year']\n",
      "1 ['among' 'annual' 'bitcoin' 'breaking' 'btc' 'cool' 'distribute' 'doge'\n",
      " 'dogecoin' 'eth' 'ethereum' 'event' 'everyone' 'giveaway' 'heretesla'\n",
      " 'join' 'news' 'om' 'part' 'take']\n",
      "2 ['elon' 'everyone' 'funny' 'joke' 'makimg' 'omg' 'tits' 'tsla']\n",
      "3 ['adop' 'analysts' 'apparently' 'assume' 'days' 'ev' 'global' 'instit'\n",
      " 'ive' 'none' 'past' 'pms' 'rt' 'talk' 'tsla']\n",
      "4 ['ad' 'bet' 'fleet' 'hertz' 'rt' 'run' 'superbowl' 'tsla']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- LDA SETUP ------------------------------------------------------------------\n",
    "\n",
    "lda_docs = vectorizer.inverse_transform(dtm)\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=lda_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]\n",
      "1 [(16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1)]\n",
      "2 [(28, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1)]\n",
      "3 [(42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1)]\n",
      "4 [(42, 1), (55, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1)]\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(lda_docs)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in lda_docs]\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.130894660949707\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- MODEL BUILDING -------------------------------------------------------------\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# LDA model building\n",
    "lda = gensim.models.ldamodel.LdaModel(\n",
    "                                corpus=doc_term_matrix,\n",
    "                                num_topics=10,\n",
    "                                id2word=dictionary\n",
    "                                )\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "# Compute Coherence Score using c_v\n",
    "#lda_cv = CoherenceModel(model=lda, corpus=doc_term_matrix, texts=lda_docs, dictionary=dictionary,\n",
    "#                        coherence='c_v', processes=1)\n",
    "#coherence_list = lda_cv.get_coherence_per_topic()\n",
    "#topic_coherence = np.asarray(coherence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.078*\"rt\" + 0.057*\"market\" + 0.038*\"nftgiveaway\" + 0.036*\"tesla\" + 0.032*\"billion\" + 0.031*\"cap\" + 0.031*\"tsla\" + 0.027*\"list\" + 0.023*\"week\" + 0.019*\"global\"'),\n",
       " (1,\n",
       "  '0.115*\"tesla\" + 0.069*\"rt\" + 0.022*\"accept\" + 0.019*\"doge\" + 0.018*\"want\" + 0.017*\"like\" + 0.017*\"get\" + 0.014*\"people\" + 0.014*\"drive\" + 0.013*\"love\"'),\n",
       " (2,\n",
       "  '0.089*\"shib\" + 0.059*\"eth\" + 0.045*\"join\" + 0.045*\"bitcoin\" + 0.043*\"btc\" + 0.041*\"twitter\" + 0.035*\"tesla\" + 0.033*\"raffle\" + 0.032*\"hold\" + 0.030*\"holders\"'),\n",
       " (3,\n",
       "  '0.075*\"tesla\" + 0.026*\"buy\" + 0.025*\"rt\" + 0.021*\"make\" + 0.017*\"car\" + 0.013*\"electric\" + 0.013*\"take\" + 0.013*\"best\" + 0.013*\"one\" + 0.012*\"get\"'),\n",
       " (4,\n",
       "  '0.089*\"tsla\" + 0.026*\"rt\" + 0.024*\"floki\" + 0.024*\"low\" + 0.016*\"stock\" + 0.012*\"production\" + 0.012*\"miss\" + 0.012*\"investment\" + 0.011*\"ethe\" + 0.010*\"call\"'),\n",
       " (5,\n",
       "  '0.055*\"rt\" + 0.052*\"tesla\" + 0.031*\"teslas\" + 0.023*\"step\" + 0.019*\"new\" + 0.018*\"years\" + 0.017*\"tsla\" + 0.015*\"hit\" + 0.015*\"gold\" + 0.014*\"include\"'),\n",
       " (6,\n",
       "  '0.055*\"tesla\" + 0.045*\"lets\" + 0.030*\"first\" + 0.026*\"rt\" + 0.024*\"team\" + 0.024*\"name\" + 0.022*\"shiba\" + 0.022*\"complete\" + 0.021*\"shibafloki\" + 0.021*\"twitt\"'),\n",
       " (7,\n",
       "  '0.051*\"elon\" + 0.042*\"musk\" + 0.035*\"tesla\" + 0.031*\"price\" + 0.030*\"rt\" + 0.019*\"one\" + 0.014*\"ever\" + 0.013*\"increase\" + 0.012*\"tax\" + 0.012*\"autopilot\"'),\n",
       " (8,\n",
       "  '0.053*\"tesla\" + 0.042*\"rt\" + 0.036*\"musks\" + 0.036*\"share\" + 0.033*\"elon\" + 0.025*\"come\" + 0.022*\"worth\" + 0.022*\"us\" + 0.017*\"trillion\" + 0.016*\"baby\"'),\n",
       " (9,\n",
       "  '0.125*\"rt\" + 0.120*\"tesla\" + 0.054*\"follow\" + 0.053*\"win\" + 0.039*\"enter\" + 0.036*\"chance\" + 0.026*\"model\" + 0.024*\"tweet\" + 0.023*\"amp\" + 0.023*\"nft\"')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_topics=20, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
