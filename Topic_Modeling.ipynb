{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import gensim\n",
    "import pandas as pd \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(all_docs):\n",
    "    no_punctuation_docs = []\n",
    "    for doc in all_docs:\n",
    "        doc = doc.replace(\"-\", \" \")\n",
    "        no_punctuation_docs.append(re.sub(r'[^\\w\\s]', '', doc))\n",
    "    return no_punctuation_docs\n",
    "\n",
    "\n",
    "#def remove_names(all_docs):\n",
    "#    no_name_docs = []\n",
    "#    for doc in all_docs:\n",
    "#        no_name_docs.append(' '.join([w for w, t in pos_tag(doc.split()) if t != 'NNP' and t != 'NNPS']))\n",
    "#    return no_name_docs\n",
    "\n",
    "\n",
    "def tokenizer(all_docs):\n",
    "    tokenized_docs = []\n",
    "    for doc in all_docs:\n",
    "        tokenized_docs.append(word_tokenize(doc))\n",
    "    return tokenized_docs\n",
    "\n",
    "\n",
    "def lemmatizer(all_docs):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_docs = []\n",
    "    for doc in all_docs:\n",
    "        temp = []\n",
    "        for token in doc:\n",
    "            if token.isalpha():\n",
    "                temp.append(wordnet_lemmatizer.lemmatize(token, \"v\"))\n",
    "        lemmatized_docs.append(temp)\n",
    "    return lemmatized_docs\n",
    "\n",
    "def untokenizer(all_docs):\n",
    "    untokenized_docs = []\n",
    "    for doc in all_docs:\n",
    "        untokenized_docs.append(\" \".join(doc))\n",
    "    return untokenized_docs\n",
    "\n",
    "\n",
    "def fetch_stop_words():\n",
    "    additional_stop_words = ['rt','get','let','dont']\n",
    "    stop_words = stopwords.words('english') + additional_stop_words\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def create_dtm(all_docs, stopwords, ngram):\n",
    "    # mdf = min(len(all_docs), 5)\n",
    "    vectorizer = CountVectorizer(lowercase=True, min_df=0.0, max_df=1.0, ngram_range=(1, ngram), stop_words=stopwords)\n",
    "    dtm = vectorizer.fit_transform(all_docs)\n",
    "    return vectorizer, dtm\n",
    "\n",
    "\n",
    "#def tfidf_transformer(dtm):\n",
    "#    tf_transformer = TfidfTransformer()\n",
    "#    tfidf = tf_transformer.fit_transform(dtm)\n",
    "#    return tf_transformer, tfidf\n",
    "\n",
    "\n",
    "#def generate_sentence_vector(tokens, model, vectorizer, tfidf_dense):\n",
    "#    vector = np.zeros(model.vector_size)\n",
    "#    for token in tokens:\n",
    "#        if token in model.wv.vocab and token in vectorizer.vocabulary_:\n",
    "#            vector = vector + model.wv[token] * tfidf_dense[0, vectorizer.vocabulary_[token]]\n",
    "#    return vector\n",
    "\n",
    "\n",
    "def list_sample(list, n=5):\n",
    "    p = min(len(list), n)\n",
    "    for idx in range(0, p):\n",
    "        print(idx, list[idx])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Mostly agree. Prices are highly elastic. At $25,000 M3/MY would have ginormous, negative margin sales. Uptake on the 0% offer was likely huge, the offer had to be withdrawn. If TSLA knew how to efficiently build cars, they would be successful.\n",
      "1 RT : Charting my list of potential leaders, a ðŸ§µ: Charts:  $AEHR $AFRM $AMBA $AMD $ASAN $BE* $COIN $LC $MQ* $NETâ€¦\n",
      "2 RT : I almost forgot! Elon Muskâ€™s three-year time out from being chairman of Tesla ends on Nov 7. Time flies when youâ€™re câ€¦\n",
      "3 RT : In case you were wondering: Yes, the muxsan ccs modification does work at Tesla superchargers. The connector and cable are aâ€¦\n",
      "4 Then the vulture arrived. Bawumia is eating every bit of it with loud noises all over, yet with little investments compared to the NDC administration. EDISON n TESLA story.\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "data = pd.read_csv('TwitterData_latest.csv')\n",
    "all_docs = data['text_clean']\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=all_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Mostly agree Prices are highly elastic At 25000 M3MY would have ginormous negative margin sales Uptake on the 0 offer was likely huge the offer had to be withdrawn If TSLA knew how to efficiently build cars they would be successful\n",
      "1 RT  Charting my list of potential leaders a  Charts  AEHR AFRM AMBA AMD ASAN BE COIN LC MQ NET\n",
      "2 RT  I almost forgot Elon Musks three year time out from being chairman of Tesla ends on Nov 7 Time flies when youre c\n",
      "3 RT  In case you were wondering Yes the muxsan ccs modification does work at Tesla superchargers The connector and cable are a\n",
      "4 Then the vulture arrived Bawumia is eating every bit of it with loud noises all over yet with little investments compared to the NDC administration EDISON n TESLA story\n"
     ]
    }
   ],
   "source": [
    "# Removing Punctuation\n",
    "no_punctuation_docs = remove_punctuation(all_docs)\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=no_punctuation_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['Mostly', 'agree', 'Prices', 'are', 'highly', 'elastic', 'At', '25000', 'M3MY', 'would', 'have', 'ginormous', 'negative', 'margin', 'sales', 'Uptake', 'on', 'the', '0', 'offer', 'was', 'likely', 'huge', 'the', 'offer', 'had', 'to', 'be', 'withdrawn', 'If', 'TSLA', 'knew', 'how', 'to', 'efficiently', 'build', 'cars', 'they', 'would', 'be', 'successful']\n",
      "1 ['RT', 'Charting', 'my', 'list', 'of', 'potential', 'leaders', 'a', 'Charts', 'AEHR', 'AFRM', 'AMBA', 'AMD', 'ASAN', 'BE', 'COIN', 'LC', 'MQ', 'NET']\n",
      "2 ['RT', 'I', 'almost', 'forgot', 'Elon', 'Musks', 'three', 'year', 'time', 'out', 'from', 'being', 'chairman', 'of', 'Tesla', 'ends', 'on', 'Nov', '7', 'Time', 'flies', 'when', 'youre', 'c']\n",
      "3 ['RT', 'In', 'case', 'you', 'were', 'wondering', 'Yes', 'the', 'muxsan', 'ccs', 'modification', 'does', 'work', 'at', 'Tesla', 'superchargers', 'The', 'connector', 'and', 'cable', 'are', 'a']\n",
      "4 ['Then', 'the', 'vulture', 'arrived', 'Bawumia', 'is', 'eating', 'every', 'bit', 'of', 'it', 'with', 'loud', 'noises', 'all', 'over', 'yet', 'with', 'little', 'investments', 'compared', 'to', 'the', 'NDC', 'administration', 'EDISON', 'n', 'TESLA', 'story']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- TOKENIZE -------------------------------------------------------------------\n",
    "\n",
    "# Tokenize each tweet\n",
    "tokenized_docs = tokenizer(no_punctuation_docs)\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['Mostly', 'agree', 'Prices', 'be', 'highly', 'elastic', 'At', 'would', 'have', 'ginormous', 'negative', 'margin', 'sales', 'Uptake', 'on', 'the', 'offer', 'be', 'likely', 'huge', 'the', 'offer', 'have', 'to', 'be', 'withdraw', 'If', 'TSLA', 'know', 'how', 'to', 'efficiently', 'build', 'cars', 'they', 'would', 'be', 'successful']\n",
      "1 ['RT', 'Charting', 'my', 'list', 'of', 'potential', 'leaders', 'a', 'Charts', 'AEHR', 'AFRM', 'AMBA', 'AMD', 'ASAN', 'BE', 'COIN', 'LC', 'MQ', 'NET']\n",
      "2 ['RT', 'I', 'almost', 'forget', 'Elon', 'Musks', 'three', 'year', 'time', 'out', 'from', 'be', 'chairman', 'of', 'Tesla', 'end', 'on', 'Nov', 'Time', 'fly', 'when', 'youre', 'c']\n",
      "3 ['RT', 'In', 'case', 'you', 'be', 'wonder', 'Yes', 'the', 'muxsan', 'ccs', 'modification', 'do', 'work', 'at', 'Tesla', 'superchargers', 'The', 'connector', 'and', 'cable', 'be', 'a']\n",
      "4 ['Then', 'the', 'vulture', 'arrive', 'Bawumia', 'be', 'eat', 'every', 'bite', 'of', 'it', 'with', 'loud', 'noise', 'all', 'over', 'yet', 'with', 'little', 'investments', 'compare', 'to', 'the', 'NDC', 'administration', 'EDISON', 'n', 'TESLA', 'story']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- LEMMATIZE ------------------------------------------------------------------\n",
    "\n",
    "# lemmatize the tokens\n",
    "lemmatized_docs = lemmatizer(tokenized_docs)\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=lemmatized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Mostly agree Prices be highly elastic At would have ginormous negative margin sales Uptake on the offer be likely huge the offer have to be withdraw If TSLA know how to efficiently build cars they would be successful\n",
      "1 RT Charting my list of potential leaders a Charts AEHR AFRM AMBA AMD ASAN BE COIN LC MQ NET\n",
      "2 RT I almost forget Elon Musks three year time out from be chairman of Tesla end on Nov Time fly when youre c\n",
      "3 RT In case you be wonder Yes the muxsan ccs modification do work at Tesla superchargers The connector and cable be a\n",
      "4 Then the vulture arrive Bawumia be eat every bite of it with loud noise all over yet with little investments compare to the NDC administration EDISON n TESLA story\n"
     ]
    }
   ],
   "source": [
    "# Untokenize the tokens to form sentence again\n",
    "untokenized_docs = untokenizer(lemmatized_docs)\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=untokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'rt', 'get', 'let', 'dont']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- STOPWORDS ------------------------------------------------------------------\n",
    "\n",
    "# Fetch stopwords from custom list\n",
    "stop_words = fetch_stop_words()\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(min_df=0.0,\n",
      "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
      "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
      "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
      "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
      "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
      "                            'itself', ...])\n",
      "0 aa\n",
      "1 aaa\n",
      "2 aaaaaaaaa\n",
      "3 aaaaaaaaaaaaaaaaaa\n",
      "4 aaaaaaah\n",
      "5 aaaaand\n",
      "6 aaaah\n",
      "7 aaaand\n",
      "8 aaahhh\n",
      "9 aabb\n",
      "10 aabout\n",
      "11 aabv\n",
      "12 aadvice\n",
      "13 aahh\n",
      "14 aaj\n",
      "15 aal\n",
      "16 aam\n",
      "17 aamazon\n",
      "18 aampe\n",
      "19 aampm\n",
      "20 aamzon\n",
      "21 aand\n",
      "22 aang\n",
      "23 aap\n",
      "24 aapex\n",
      "25 aapl\n",
      "26 aaplampamzn\n",
      "27 aaplamzn\n",
      "28 aapldisba\n",
      "29 aaple\n",
      "30 aaplo\n",
      "31 aapls\n",
      "32 aapple\n",
      "33 aapt\n",
      "34 aar\n",
      "35 aare\n",
      "36 aaron\n",
      "37 aaronrogers\n",
      "38 aary\n",
      "39 aas\n",
      "40 aatma\n",
      "41 aattracctive\n",
      "42 aattractiv\n",
      "43 aav\n",
      "44 aava\n",
      "45 aave\n",
      "46 aaww\n",
      "47 aaya\n",
      "48 aayengi\n",
      "49 ab\n",
      "(172143, 43151)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- VECTORIZE ------------------------------------------------------------------\n",
    "#untokenized_docs = no_punctuation_docs \n",
    "# Vectorize words\n",
    "vectorizer, dtm = create_dtm(untokenized_docs, stop_words, 1)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "if verbose:\n",
    "    print(vectorizer)\n",
    "    list_sample(list=feature_names, n=50)\n",
    "    print(dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- TFIDF ----------------------------------------------------------------------\n",
    "\n",
    "# tfidf transformation\n",
    "#tf_transformer, tfidf = tfidf_transformer(dtm)\n",
    "\n",
    "#if verbose:\n",
    "#    print(tf_transformer)\n",
    "#    print(tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['mostly' 'agree' 'prices' 'highly' 'elastic' 'would' 'ginormous'\n",
      " 'negative' 'margin' 'sales' 'uptake' 'offer' 'likely' 'huge' 'withdraw'\n",
      " 'tsla' 'know' 'efficiently' 'build' 'cars' 'successful']\n",
      "1 ['charting' 'list' 'potential' 'leaders' 'charts' 'aehr' 'afrm' 'amba'\n",
      " 'amd' 'asan' 'coin' 'lc' 'mq' 'net']\n",
      "2 ['almost' 'forget' 'elon' 'musks' 'three' 'year' 'time' 'chairman' 'tesla'\n",
      " 'end' 'nov' 'fly' 'youre']\n",
      "3 ['tesla' 'case' 'wonder' 'yes' 'muxsan' 'ccs' 'modification' 'work'\n",
      " 'superchargers' 'connector' 'cable']\n",
      "4 ['tesla' 'vulture' 'arrive' 'bawumia' 'eat' 'every' 'bite' 'loud' 'noise'\n",
      " 'yet' 'little' 'investments' 'compare' 'ndc' 'administration' 'edison'\n",
      " 'story']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- LDA SETUP ------------------------------------------------------------------\n",
    "\n",
    "lda_docs = vectorizer.inverse_transform(dtm)\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=lda_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)]\n",
      "1 [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]\n",
      "2 [(35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1)]\n",
      "3 [(43, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1)]\n",
      "4 [(43, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1)]\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(lda_docs)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in lda_docs]\n",
    "\n",
    "if verbose:\n",
    "    list_sample(list=doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.58522081375122\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------- MODEL BUILDING -------------------------------------------------------------\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# LDA model building\n",
    "lda = gensim.models.ldamodel.LdaModel(\n",
    "                                corpus=doc_term_matrix,\n",
    "                                num_topics=10,\n",
    "                                id2word=dictionary\n",
    "                                )\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "# Compute Coherence Score using c_v\n",
    "#lda_cv = CoherenceModel(model=lda, corpus=doc_term_matrix, texts=lda_docs, dictionary=dictionary,\n",
    "#                        coherence='c_v', processes=1)\n",
    "#coherence_list = lda_cv.get_coherence_per_topic()\n",
    "#topic_coherence = np.asarray(coherence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.043*\"tsla\" + 0.024*\"earn\" + 0.021*\"aapl\" + 0.020*\"eth\" + 0.017*\"official\" + 0.017*\"lcid\" + 0.014*\"amzn\" + 0.014*\"nft\" + 0.014*\"fb\" + 0.013*\"market\"'),\n",
       " (1,\n",
       "  '0.086*\"tesla\" + 0.023*\"elon\" + 0.023*\"musk\" + 0.017*\"like\" + 0.011*\"see\" + 0.011*\"make\" + 0.011*\"happen\" + 0.011*\"want\" + 0.010*\"retweet\" + 0.010*\"work\"'),\n",
       " (2,\n",
       "  '0.081*\"tesla\" + 0.062*\"giveaway\" + 0.055*\"amp\" + 0.054*\"doge\" + 0.045*\"announce\" + 0.042*\"winner\" + 0.042*\"follow\" + 0.041*\"check\" + 0.041*\"hours\" + 0.031*\"person\"'),\n",
       " (3,\n",
       "  '0.069*\"tesla\" + 0.031*\"one\" + 0.026*\"cars\" + 0.024*\"buy\" + 0.017*\"call\" + 0.016*\"price\" + 0.016*\"model\" + 0.016*\"electric\" + 0.014*\"charge\" + 0.013*\"hertz\"'),\n",
       " (4,\n",
       "  '0.080*\"stock\" + 0.074*\"tesla\" + 0.058*\"billion\" + 0.052*\"sell\" + 0.042*\"world\" + 0.041*\"twitter\" + 0.036*\"right\" + 0.031*\"musk\" + 0.024*\"solve\" + 0.023*\"elon\"'),\n",
       " (5,\n",
       "  '0.114*\"tsla\" + 0.032*\"stock\" + 0.021*\"new\" + 0.018*\"tesla\" + 0.018*\"hit\" + 0.017*\"long\" + 0.013*\"today\" + 0.013*\"go\" + 0.012*\"value\" + 0.012*\"short\"'),\n",
       " (6,\n",
       "  '0.078*\"tesla\" + 0.026*\"tax\" + 0.022*\"back\" + 0.014*\"credit\" + 0.014*\"bring\" + 0.012*\"ferry\" + 0.010*\"gleneagles\" + 0.010*\"teslas\" + 0.010*\"lfp\" + 0.010*\"fsd\"'),\n",
       " (7,\n",
       "  '0.051*\"tesla\" + 0.031*\"tsla\" + 0.022*\"market\" + 0.019*\"make\" + 0.019*\"company\" + 0.019*\"week\" + 0.017*\"go\" + 0.017*\"gain\" + 0.015*\"trillion\" + 0.014*\"day\"'),\n",
       " (8,\n",
       "  '0.087*\"tesla\" + 0.045*\"dogecoin\" + 0.043*\"shib\" + 0.036*\"accept\" + 0.035*\"bitcoin\" + 0.032*\"like\" + 0.027*\"moon\" + 0.017*\"elonmusk\" + 0.017*\"btc\" + 0.017*\"soon\"'),\n",
       " (9,\n",
       "  '0.053*\"give\" + 0.047*\"tesla\" + 0.045*\"baby\" + 0.044*\"update\" + 0.042*\"first\" + 0.042*\"contest\" + 0.040*\"tweet\" + 0.035*\"add\" + 0.034*\"free\" + 0.033*\"giveaway\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_topics=20, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
